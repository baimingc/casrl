{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import yaml\n",
    "from utils import dumb_reward_plot\n",
    "import gym\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "sys.path.append('./envs/cartpole-envs')\n",
    "sys.path.append('./')\n",
    "import cartpole_envs\n",
    "\n",
    "from utils import plot_reward, plot_index\n",
    "from mpc.mpc_cp import MPC\n",
    "from baselines.NP_epi import NP\n",
    "\n",
    "def prepare_dynamics(gym_config):\n",
    "    dynamics_name = gym_config['dynamics_name']\n",
    "    seed = gym_config['seed']\n",
    "    dynamics_set = []\n",
    "    for i in range(len(dynamics_name)):\n",
    "        dynamics_set.append(gym.make(dynamics_name[i]))\n",
    "    task = [dynamics_set[i] for i in gym_config['task_dynamics_list']]\n",
    "    return task\n",
    "\n",
    "def load_config(config_path=\"config.yml\"):\n",
    "    if os.path.isfile(config_path):\n",
    "        f = open(config_path)\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)\n",
    "    else:\n",
    "        raise Exception(\"Configuration file is not found in the path: \"+config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-10 10:34:53.904 | INFO     | __main__:<module>:11 - Using model: NP\n"
     ]
    }
   ],
   "source": [
    "# config = load_config('config/config_cpstable_np.yml')\n",
    "config = load_config('config/config_swingup_robust.yml')\n",
    "mpc_config = config['mpc_config']\n",
    "gym_config = config['gym_config']\n",
    "render = gym_config['render']\n",
    "np_config = config['NP_config']\n",
    "\n",
    "model = NP(NP_config=np_config)\n",
    "model.model.load_state_dict(torch.load( './misc/log/robust_model_latent-10-09-22.pth'))\n",
    "\n",
    "logger.info('Using model: {}', model.name)\n",
    "\n",
    "mpc_controller = MPC(mpc_config=mpc_config)\n",
    "\n",
    "# prepare task\n",
    "task = prepare_dynamics(gym_config)\n",
    "# print(gym_config)\n",
    "\n",
    "\"\"\"start DPGP-MBRL\"\"\"\n",
    "data_buffer = []\n",
    "label_list = []\n",
    "subtask_list = []\n",
    "subtask_reward = []\n",
    "subtask_succ_count = [0]\n",
    "comp_trainable = [1]\n",
    "task_reward = []\n",
    "trainable = True\n",
    "task_solved = False\n",
    "subtask_solved = [False, False, False, False]\n",
    "total_count = 0\n",
    "task_epi = 0\n",
    "log_name = None\n",
    "\n",
    "total_tasks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9572, -2.1486]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.3 pole_length:  0.3 step:  200 acc_reward:  18.260323161035963 violation_rate:  0.025\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1015, -2.3212]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.3 pole_length:  0.4 step:  200 acc_reward:  74.24123189388234 violation_rate:  0.025\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.2974, -2.5556]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.3 pole_length:  0.5 step:  200 acc_reward:  171.82084740847353 violation_rate:  0.155\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4597, -2.7497]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.3 pole_length:  0.6 step:  200 acc_reward:  146.30674727379574 violation_rate:  0.15\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5108, -2.8108]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  131.5912171639975 violation_rate:  0.185\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9667, -2.1600]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.4 pole_length:  0.3 step:  200 acc_reward:  32.60876643942077 violation_rate:  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1267, -2.3514]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.4 pole_length:  0.4 step:  200 acc_reward:  101.66976563204865 violation_rate:  0.16\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3270, -2.5910]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.4 pole_length:  0.5 step:  200 acc_reward:  157.51369796747525 violation_rate:  0.17\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4673, -2.7588]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.4 pole_length:  0.6 step:  200 acc_reward:  158.55458613580637 violation_rate:  0.275\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5290, -2.8326]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.4 pole_length:  0.7 step:  200 acc_reward:  142.56063133246784 violation_rate:  0.235\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9756, -2.1707]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.5 pole_length:  0.3 step:  200 acc_reward:  41.38152427394677 violation_rate:  0.025\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1249, -2.3492]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.5 pole_length:  0.4 step:  200 acc_reward:  105.99764462427578 violation_rate:  0.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.3495, -2.6179]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.5 pole_length:  0.5 step:  200 acc_reward:  177.47892027887727 violation_rate:  0.18\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5091, -2.8088]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.5 pole_length:  0.6 step:  200 acc_reward:  154.92681183020048 violation_rate:  0.155\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5969, -2.9138]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.5 pole_length:  0.7 step:  200 acc_reward:  154.66531619852412 violation_rate:  0.245\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.9934, -2.1919]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.6 pole_length:  0.3 step:  200 acc_reward:  41.229395453478325 violation_rate:  0.0\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1720, -2.4056]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.6 pole_length:  0.4 step:  200 acc_reward:  68.15204257889688 violation_rate:  0.125\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4156, -2.6969]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.6 pole_length:  0.5 step:  200 acc_reward:  140.4757964228001 violation_rate:  0.155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.5953, -2.9118]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.6 pole_length:  0.6 step:  200 acc_reward:  129.32863175424262 violation_rate:  0.27\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6930, -3.0287]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.6 pole_length:  0.7 step:  200 acc_reward:  120.2626032518041 violation_rate:  0.22\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.0148, -2.2175]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.7 pole_length:  0.3 step:  200 acc_reward:  42.999765767891155 violation_rate:  0.0\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.1859, -2.4222]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.7 pole_length:  0.4 step:  200 acc_reward:  60.63512340919433 violation_rate:  0.02\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.4710, -2.7632]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.7 pole_length:  0.5 step:  200 acc_reward:  140.7256103025481 violation_rate:  0.2\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.6837, -3.0176]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.7 pole_length:  0.6 step:  200 acc_reward:  125.60014422853054 violation_rate:  0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 2.7636, -3.1132]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "pole_mass:  0.7 pole_length:  0.7 step:  200 acc_reward:  44.52676402975834 violation_rate:  0.09\n"
     ]
    }
   ],
   "source": [
    "config = load_config('config/config_swingup_robust.yml')\n",
    "mpc_config = config['mpc_config']\n",
    "mpc_controller = MPC(mpc_config=mpc_config)\n",
    "\"\"\"testing the model with MPC while training \"\"\"\n",
    "test_episode = 1\n",
    "test_epoch = 10\n",
    "log = []\n",
    "m_p_list = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "l_list = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "for m_p in m_p_list:\n",
    "    for l in l_list:\n",
    "        task_steps = 0\n",
    "        env = task[0]\n",
    "#         m_p = m_p_list[np.random.randint(2)]\n",
    "#         l = l_list[np.random.randint(2)]\n",
    "        env.unwrapped.m_p = m_p\n",
    "        env.unwrapped.l = l\n",
    "        for epi in range(test_episode):\n",
    "            acc_reward = 0\n",
    "            obs = env.reset()\n",
    "            O, A, R, acc_reward, done, V = [], [], [], 0, False, []\n",
    "            mpc_controller.reset()\n",
    "            i = 0\n",
    "            while not done:\n",
    "                i+= 1\n",
    "                env_copy = prepare_dynamics(gym_config)[0]\n",
    "                env_copy.unwrapped.m_p = m_p\n",
    "                env_copy.unwrapped.l = l\n",
    "                env_copy.reset()\n",
    "                if task_steps > 0:\n",
    "                    action = np.array([mpc_controller.act(task=env_copy, model=model, state=obs, ground_truth=True)])\n",
    "                else:\n",
    "                    action = np.array([0.0])\n",
    "                obs_next, reward, done, violation = env.step(action)\n",
    "                task_steps += 1\n",
    "                A.append(action)\n",
    "                O.append(obs_next)\n",
    "                R.append(reward)\n",
    "                V.append(violation)\n",
    "\n",
    "                model.data_process([0, obs, action, obs_next - obs])\n",
    "                obs = obs_next\n",
    "                acc_reward += reward\n",
    "#             print('task: ', task_idx,'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            print('pole_mass: ', m_p, 'pole_length: ', l, 'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            env.close()\n",
    "\n",
    "            if done:\n",
    "                samples = {\n",
    "                    \"obs\": np.array(O),\n",
    "                    \"actions\": np.array(A),\n",
    "                    \"rewards\": np.array(R),\n",
    "                    \"reward_sum\": acc_reward,\n",
    "                    \"violation_rate\": sum(V)/len(V)\n",
    "                }\n",
    "                log.append(samples)\n",
    "#                 if log_name is None:\n",
    "#                     log_name = datetime.datetime.now()\n",
    "#                 path = './misc/log/np_adaptation' + log_name.strftime(\"%d-%H-%M\") + '.npy'\n",
    "#                 np.save(path, log, allow_pickle=True)\n",
    "#                 dumb_reward_plot(path)\n",
    "            \n",
    "        model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
