{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "from DPNNMM import DPNNMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 0 has data 1117\n",
      "task 1 has data 907\n",
      "task 2 has data 1480\n",
      "task 3 has data 1386\n"
     ]
    }
   ],
   "source": [
    "def load_config(config_path=\"config.yml\"):\n",
    "    if os.path.isfile(config_path):\n",
    "        f = open(config_path)\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)\n",
    "    else:\n",
    "        raise Exception(\"Configuration file is not found in the path: \"+config_path)\n",
    "\n",
    "config = load_config('config_test.yml')\n",
    "nn_config = config['NN_config']\n",
    "mpc_config = config['mpc_config']\n",
    "gym_config = config['gym_config']\n",
    "dp_config = config[\"DP_config\"]\n",
    "\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data_dict = {0:[],1:[],2:[],3:[]}\n",
    "for d in data:\n",
    "    task_idx, obs, action, label = d\n",
    "    data_dict[task_idx].append(d)\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    print(\"task %d has data %d\"%(key, len(data_dict[key])))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch [99/100],loss train: -5.8096/0.5904, loss test  -5.8582/0.4848\n",
      "training epoch [99/100],loss train: -7.2418/0.3547, loss test  -6.9613/0.4989\n",
      "training epoch [99/100],loss train: -7.4842/0.1431, loss test  -7.3545/0.1527\n",
      "training epoch [99/100],loss train: -7.9301/0.0687, loss test  -7.7426/0.0358\n",
      "training epoch [99/100],loss train: -8.3418/0.0436, loss test  -8.1212/0.0431\n",
      "training epoch [99/100],loss train: -8.8399/0.0246, loss test  -7.8157/0.0357\n",
      "training epoch [99/100],loss train: -8.4985/0.0214, loss test  -7.7482/0.0405\n",
      "training epoch [99/100],loss train: -8.7260/0.0200, loss test  -8.1746/0.0180\n",
      "training epoch [99/100],loss train: -8.8476/0.0182, loss test  -7.9039/0.0376\n",
      "training epoch [99/100],loss train: -8.7216/0.0173, loss test  -8.0827/0.0309\n",
      "training epoch [99/100],loss train: -9.1407/0.0166, loss test  -7.8659/0.0172\n",
      "training epoch [99/100],loss train: -9.0658/0.0146, loss test  -8.5367/0.0187\n",
      "training epoch [99/100],loss train: -9.1676/0.0154, loss test  -8.6391/0.0143\n",
      "training epoch [99/100],loss train: -8.4942/0.0198, loss test  -8.1175/0.0215\n",
      "training epoch [99/100],loss train: -9.0443/0.0132, loss test  -8.4449/0.0225\n",
      "training epoch [99/100],loss train: -9.4873/0.0116, loss test  -9.1316/0.0156\n",
      "training epoch [99/100],loss train: -9.0146/0.0145, loss test  -8.2616/0.0164\n",
      "training epoch [99/100],loss train: -8.2654/0.0371, loss test  -7.5838/0.0669\n",
      "training epoch [99/100],loss train: -9.3147/0.0156, loss test  -7.8463/0.0348\n",
      "training epoch [99/100],loss train: -9.4757/0.0123, loss test  -8.1587/0.0154\n"
     ]
    }
   ],
   "source": [
    "# meta fit to get a prior model\n",
    "torch.manual_seed(0)\n",
    "model = DPNNMM(dp_config, nn_config)\n",
    "\n",
    "dataset = []\n",
    "for i in range(3):\n",
    "    dataset += data_dict[i][::2]\n",
    "for j in range(20):\n",
    "    model.meta_fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************** alpha: 1   ***************\n",
      "stm len:  100\n",
      "rho old : [] rho_new:  [21301.4765625]\n",
      "training epoch [99/100],loss train: 17.0688/0.0125, loss test  6.8810/0.0124\n",
      "pred task:  0 true: 0\n",
      "stm len:  100\n",
      "rho old : [41333.8125] rho_new:  [21178.857421875]\n",
      "training epoch [99/100],loss train: -11.7707/0.0030, loss test  2.9556/0.0645\n",
      "pred task:  0 true: 0\n",
      "stm len:  100\n",
      "rho old : [61394.09375] rho_new:  [25549.64453125]\n",
      "training epoch [99/100],loss train: -8.5057/0.1282, loss test  -7.4906/0.0331\n",
      "pred task:  0 true: 0\n",
      "stm len:  74\n",
      "rho old : [6980.49365234375] rho_new:  [18677.193359375]\n",
      "training epoch [99/100],loss train: -9.6876/0.0034, loss test  -8.8114/0.0136\n",
      "pred task:  1 true: 1\n",
      "stm len:  100\n",
      "rho old : [9543.2099609375, 13724.0546875] rho_new:  [34300.203125]\n",
      "training epoch [99/100],loss train: -12.6563/0.0000, loss test  -2.1655/0.0042\n",
      "pred task:  2 true: 1\n",
      "stm len:  100\n",
      "rho old : [8536.224609375, 16291.501953125, 62574.80859375] rho_new:  [36914.68359375]\n",
      "training epoch [99/100],loss train: -12.5958/0.0000, loss test  -3.3200/0.0037\n",
      "pred task:  2 true: 1\n",
      "stm len:  100\n",
      "rho old : [7692.66650390625, 11406.779296875, 69485.390625] rho_new:  [24553.8046875]\n",
      "training epoch [99/100],loss train: -12.3160/0.0002, loss test  -7.2787/0.0022\n",
      "pred task:  2 true: 1\n",
      "stm len:  3\n",
      "rho old : [4654.68310546875, 13712.3076171875, 30425.630859375] rho_new:  [20227.40625]\n",
      "training epoch [99/100],loss train: -11.8770/0.0005, loss test  -9.9684/0.0022\n",
      "pred task:  2 true: 2\n",
      "stm len:  100\n",
      "rho old : [2267.926025390625, 7557.23193359375, 3328.708740234375] rho_new:  [21125.650390625]\n",
      "training epoch [99/100],loss train: -11.0409/0.0054, loss test  -7.7954/0.0126\n",
      "pred task:  3 true: 2\n",
      "stm len:  100\n",
      "rho old : [2574.796142578125, 5902.8955078125, 4109.541015625, 29702.39453125] rho_new:  [12445.8974609375]\n",
      "training epoch [99/100],loss train: -10.1719/0.0081, loss test  -3.7083/0.0117\n",
      "pred task:  3 true: 2\n",
      "stm len:  100\n",
      "rho old : [3705.144287109375, 10063.50390625, 3483.475830078125, 36221.41015625] rho_new:  [25307.0703125]\n",
      "training epoch [99/100],loss train: -8.9279/0.0780, loss test  -5.6856/0.2312\n",
      "pred task:  3 true: 2\n",
      "stm len:  100\n",
      "rho old : [2933.49609375, 7332.35546875, 1726.98291015625, 10836.4814453125] rho_new:  [18624.4140625]\n",
      "training epoch [99/100],loss train: -8.6114/0.0727, loss test  -5.0015/0.0863\n",
      "pred task:  4 true: 2\n",
      "stm len:  94\n",
      "rho old : [4192.2578125, 9898.49609375, 5190.064453125, 15303.876953125, 28068.333984375] rho_new:  [25502.05859375]\n",
      "training epoch [99/100],loss train: -10.3890/0.0111, loss test  -7.5545/0.0066\n",
      "pred task:  4 true: 3\n",
      "stm len:  100\n",
      "rho old : [1668.2857666015625, 2951.243408203125, 17786.912109375, 4357.494140625, 3942.49609375] rho_new:  [3585.0224609375]\n",
      "training epoch [99/100],loss train: -10.3591/0.0036, loss test  1.6737/0.0186\n",
      "pred task:  2 true: 3\n",
      "stm len:  100\n",
      "rho old : [2292.760986328125, 3714.341552734375, 9402.552734375, 8176.50439453125, 6436.98486328125] rho_new:  [5288.1806640625]\n",
      "training epoch [99/100],loss train: -10.0567/0.0052, loss test  -9.2885/0.0074\n",
      "pred task:  2 true: 3\n",
      "stm len:  100\n",
      "rho old : [3150.237060546875, 3543.77587890625, 17760.994140625, 9055.224609375, 12698.1845703125] rho_new:  [7350.35986328125]\n",
      "training epoch [99/100],loss train: -10.1585/0.0052, loss test  -3.0316/0.0136\n",
      "pred task:  2 true: 3\n",
      "stm len:  100\n",
      "rho old : [2739.597412109375, 3451.47119140625, 17154.40234375, 7930.75537109375, 9945.0224609375] rho_new:  [6139.93212890625]\n",
      "training epoch [99/100],loss train: -10.4496/0.0043, loss test  -7.6734/0.0074\n",
      "pred task:  2 true: 3\n"
     ]
    }
   ],
   "source": [
    "for alpha in [1]:\n",
    "    print(\"**************** alpha:\", alpha,\"  ***************\")\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    dp_config[\"alpha\"] = alpha\n",
    "    model = DPNNMM(dp_config, nn_config)\n",
    "\n",
    "    dataset = []\n",
    "    for i in range(4):\n",
    "        dataset += data_dict[i]\n",
    "    model.meta_load(\"meta_model.pth\")\n",
    "    #model.meta_fit(dataset)\n",
    "\n",
    "    task_idx_old = 0\n",
    "    pred = []\n",
    "    true_task_idx = []\n",
    "    times = []\n",
    "    for task_idx in range(4):\n",
    "        for d in data_dict[task_idx][::3]:\n",
    "            model.add_data_point(d)\n",
    "            if model.stm_is_full or task_idx!=task_idx_old:\n",
    "                print(\"stm len: \", len(model.stm))\n",
    "                time_use, task_idx_pred = model.fit()\n",
    "                pred.append(task_idx_pred)\n",
    "                true_task_idx.append(task_idx)\n",
    "                times.append(time_use)\n",
    "\n",
    "                print(\"pred task: \", task_idx_pred, \"true:\", task_idx)\n",
    "            task_idx_old = task_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stm len:  100\n",
      "rho old : [4456.3427734375, 6100.25, 8694.8486328125, 3148.8525390625, 2424.306884765625] rho_new:  [10137.5615234375]\n",
      "training epoch [99/100],loss train: -10.5626/0.0099, loss test  42.7360/0.0567\n",
      "pred task:  5 true: 0\n",
      "stm len:  100\n",
      "rho old : [13761.3955078125, 15854.2333984375, 6922.10498046875, 6073.41259765625, 12035.2470703125, 24664.046875] rho_new:  [21863.71484375]\n",
      "training epoch [99/100],loss train: -9.5219/0.0108, loss test  2.1573/0.0568\n",
      "pred task:  5 true: 0\n",
      "stm len:  100\n",
      "rho old : [10069.25390625, 9543.2783203125, 595.9694213867188, 3955.014404296875, 11283.4296875, 19680.705078125] rho_new:  [17385.03125]\n",
      "training epoch [99/100],loss train: -9.8160/0.0119, loss test  -5.2260/0.0591\n",
      "pred task:  5 true: 0\n",
      "stm len:  100\n",
      "rho old : [11200.978515625, 17109.599609375, 3433.751708984375, 7880.57861328125, 11870.7548828125, 36862.5703125] rho_new:  [24329.392578125]\n",
      "training epoch [99/100],loss train: -10.1058/0.0173, loss test  -5.4354/0.0265\n",
      "pred task:  5 true: 0\n",
      "stm len:  35\n",
      "rho old : [6513.5517578125, 23287.482421875, 3448.54541015625, 4356.8095703125, 12588.3876953125, 25690.64453125] rho_new:  [25195.34375]\n",
      "training epoch [99/100],loss train: -9.5824/0.0168, loss test  -7.6472/0.0330\n",
      "pred task:  5 true: 1\n",
      "stm len:  100\n",
      "rho old : [9543.2099609375, 13724.0546875, 94125.640625, 6069.5458984375, 17753.78125, 13013.2626953125] rho_new:  [34300.203125]\n",
      "training epoch [99/100],loss train: -10.4424/0.0048, loss test  -7.9075/0.0154\n",
      "pred task:  2 true: 1\n",
      "stm len:  100\n",
      "rho old : [8536.224609375, 16291.501953125, 90503.6953125, 10391.2373046875, 16257.6171875, 13800.099609375] rho_new:  [36914.68359375]\n",
      "training epoch [99/100],loss train: -10.0425/0.0049, loss test  -8.8668/0.0064\n",
      "pred task:  2 true: 1\n",
      "stm len:  100\n",
      "rho old : [7692.66650390625, 11406.779296875, 57929.20703125, 5263.76123046875, 7588.462890625, 7352.724609375] rho_new:  [24553.8046875]\n",
      "training epoch [99/100],loss train: -10.6535/0.0037, loss test  -6.1182/0.0074\n",
      "pred task:  2 true: 1\n",
      "stm len:  3\n",
      "rho old : [4654.68310546875, 13712.3076171875, 116344.640625, 18026.443359375, 8581.30078125, 10012.9599609375] rho_new:  [20227.40625]\n",
      "training epoch [99/100],loss train: -10.3578/0.0044, loss test  -9.3256/0.0047\n",
      "pred task:  2 true: 2\n",
      "stm len:  100\n",
      "rho old : [2267.926025390625, 7557.23193359375, 4887.046875, 17442.802734375, 28807.294921875, 12513.0751953125] rho_new:  [21125.650390625]\n",
      "training epoch [99/100],loss train: -10.7255/0.0028, loss test  -1.7593/0.0124\n",
      "pred task:  4 true: 2\n",
      "stm len:  100\n",
      "rho old : [2574.796142578125, 5902.8955078125, 2268.10546875, 13894.6025390625, 42718.91015625, 8698.203125] rho_new:  [12445.8974609375]\n",
      "training epoch [99/100],loss train: -10.4972/0.0069, loss test  -8.7063/0.0141\n",
      "pred task:  4 true: 2\n",
      "stm len:  100\n",
      "rho old : [3705.144287109375, 10063.50390625, 2395.12646484375, 17928.794921875, 36372.22265625, 9956.0244140625] rho_new:  [25307.0703125]\n",
      "training epoch [99/100],loss train: -10.1306/0.0118, loss test  -7.0489/0.0129\n",
      "pred task:  4 true: 2\n",
      "stm len:  100\n",
      "rho old : [2933.49609375, 7332.35546875, 2989.579345703125, 10836.4814453125, 112635.734375, 6027.60595703125] rho_new:  [18624.4140625]\n",
      "training epoch [99/100],loss train: -11.0379/0.0053, loss test  -7.0867/0.0074\n",
      "pred task:  4 true: 2\n",
      "stm len:  94\n",
      "rho old : [4192.2578125, 9898.49609375, 1977.33642578125, 15303.876953125, 137122.03125, 19023.20703125] rho_new:  [25502.05859375]\n",
      "training epoch [99/100],loss train: -10.8824/0.0061, loss test  -10.0820/0.0073\n",
      "pred task:  4 true: 3\n",
      "stm len:  100\n",
      "rho old : [1668.2857666015625, 2951.243408203125, 46886.52734375, 4357.494140625, 5642.71875, 3256.94580078125] rho_new:  [3585.0224609375]\n",
      "training epoch [99/100],loss train: -10.4551/0.0038, loss test  -7.4997/0.0063\n",
      "pred task:  2 true: 3\n",
      "stm len:  100\n",
      "rho old : [2292.760986328125, 3714.341552734375, 68291.96875, 8176.50439453125, 6955.13916015625, 6739.712890625] rho_new:  [5288.1806640625]\n",
      "training epoch [99/100],loss train: -10.9257/0.0032, loss test  -9.7148/0.0049\n",
      "pred task:  2 true: 3\n",
      "stm len:  100\n",
      "rho old : [3150.237060546875, 3543.77587890625, 91481.390625, 9055.224609375, 7120.09228515625, 5213.21533203125] rho_new:  [7350.35986328125]\n",
      "training epoch [99/100],loss train: -10.5971/0.0033, loss test  -9.8751/0.0041\n",
      "pred task:  2 true: 3\n",
      "stm len:  100\n",
      "rho old : [2739.597412109375, 3451.47119140625, 99101.234375, 7930.75537109375, 6404.48046875, 597.7508544921875] rho_new:  [6139.93212890625]\n",
      "training epoch [99/100],loss train: -9.5487/0.0050, loss test  -8.8364/0.0049\n",
      "pred task:  2 true: 3\n"
     ]
    }
   ],
   "source": [
    "task_idx_old = 0\n",
    "pred = []\n",
    "true_task_idx = []\n",
    "times = []\n",
    "for task_idx in range(4):\n",
    "    for d in data_dict[task_idx][::3]:\n",
    "        model.add_data_point(d)\n",
    "        if model.stm_is_full or task_idx!=task_idx_old:\n",
    "            print(\"stm len: \", len(model.stm))\n",
    "            time_use, task_idx_pred = model.fit()\n",
    "            pred.append(task_idx_pred)\n",
    "            true_task_idx.append(task_idx)\n",
    "            times.append(time_use)\n",
    "\n",
    "            print(\"pred task: \", task_idx_pred, \"true:\", task_idx)\n",
    "        task_idx_old = task_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_idx_old = 0\n",
    "pred = []\n",
    "true_task_idx = []\n",
    "times = []\n",
    "for task_idx in range(4):\n",
    "    for d in data_dict[task_idx][200:]:\n",
    "        model.add_data_point(d)\n",
    "        if model.stm_is_full or task_idx!=task_idx_old:\n",
    "            time_use, task_idx_pred = model.fit()\n",
    "            pred.append(task_idx_pred)\n",
    "            true_task_idx.append(task_idx)\n",
    "            times.append(time_use)\n",
    "            \n",
    "            print(\"pred task: \", task_idx_pred, \"true:\", task_idx)\n",
    "        task_idx_old = task_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta training phase\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = NNComponent(NN_config=nn_config)\n",
    "for i in range(4):\n",
    "    for d in data_dict[i]:\n",
    "        model.add_data_point(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    test_data_loader, _ = model.make_dataset(data_dict[i])\n",
    "    nll, mse = model.validate_model(test_data_loader)\n",
    "    print(\"task \",i,\"nll: \", nll, \"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_epochs = 500\n",
    "model.fit(data_dict[2][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    test_data_loader, _ = model.make_dataset(data_dict[i][100:])\n",
    "    nll, mse = model.validate_model(test_data_loader)\n",
    "    print(\"task \",i,\"nll: \", nll, \"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = []\n",
    "for d in data_dict[0]:\n",
    "    s = [d[1]]\n",
    "    a = [d[2]]\n",
    "    mean, var = model.nn_meta_model.predict(s, a)\n",
    "    var_list.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = []\n",
    "n_comps = len(comps)\n",
    "rho_old = [comps[k] for k in range(n_comps)]\n",
    "n_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def CUDA(var):\n",
    "    return var.cuda() if torch.cuda.is_available() else var\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input=7, n_output=6, n_h=2, size_h=128):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.fc_in = nn.Linear(n_input, size_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_list = nn.ModuleList()\n",
    "        for i in range(n_h - 1):\n",
    "            self.fc_list.append(nn.Linear(size_h, size_h))\n",
    "        \n",
    "        #self.fc_out = nn.Linear(size_h, n_output)\n",
    "        self.fc_out_mean = nn.Linear(size_h, n_output)\n",
    "        self.fc_out_var = nn.Linear(size_h, n_output)\n",
    "        \n",
    "        # Initialize weight\n",
    "        nn.init.normal_(self.fc_in.weight, 0.0, 0.02)\n",
    "        nn.init.normal_(self.fc_out_mean.weight, 0.0, 0.02)\n",
    "        nn.init.normal_(self.fc_out_var.weight, 0.0, 0.02)\n",
    "        \n",
    "        self.fc_list.apply(self.init_normal)\n",
    "        \n",
    "        \n",
    "\n",
    "    def init_normal(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(-1, self.n_input)\n",
    "        out = self.fc_in(out)\n",
    "        out = self.relu(out)\n",
    "        for _, layer in enumerate(self.fc_list, start=0):\n",
    "            out = layer(out)\n",
    "            out = self.relu(out)\n",
    "        out_mean = self.fc_out_mean(out)\n",
    "        out_var = self.fc_out_var(out)\n",
    "        out_var = self.relu(out_var)\n",
    "        out_var = out_var + 0.001 # add a small bias to make sure it is not equal to 0\n",
    "        return (out_mean, out_var)\n",
    "\n",
    "class NNComponent(object):\n",
    "    # output: [state mean, state var]\n",
    "    name = \"NN\"\n",
    "    def __init__(self, NN_config):\n",
    "        super().__init__()\n",
    "        model_config = NN_config[\"model_config\"]\n",
    "        training_config = NN_config[\"training_config\"]\n",
    "        \n",
    "        self.state_dim = model_config[\"state_dim\"]\n",
    "        self.action_dim = model_config[\"action_dim\"]\n",
    "        self.input_dim = self.state_dim+self.action_dim\n",
    "\n",
    "        self.n_epochs = training_config[\"n_epochs\"]\n",
    "        self.lr = training_config[\"learning_rate\"]\n",
    "        self.batch_size = training_config[\"batch_size\"]\n",
    "        \n",
    "        self.save_model_flag = training_config[\"save_model_flag\"]\n",
    "        self.save_model_path = training_config[\"save_model_path\"]\n",
    "        \n",
    "        self.validation_flag = training_config[\"validation_flag\"]\n",
    "        self.validate_freq = training_config[\"validation_freq\"]\n",
    "        self.validation_ratio = training_config[\"validation_ratio\"]\n",
    "\n",
    "        if model_config[\"load_model\"]:\n",
    "            self.model = CUDA(torch.load(model_config[\"model_path\"]))\n",
    "        else:\n",
    "            self.model = CUDA(MLP(self.input_dim, self.state_dim, model_config[\"hidden_dim\"], model_config[\"hidden_size\"]))\n",
    "\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        #self.data = None\n",
    "        #self.label = None\n",
    "        self.dataset = []\n",
    "\n",
    "    def criterion(self, output, label):\n",
    "        nll = -self.log_likelihood(output, label) # [batch]\n",
    "        return torch.mean(nll)\n",
    "\n",
    "    def log_likelihood(self, output, label):\n",
    "        mu = output[0] # [batch, state_dim]\n",
    "        var = output[1] # [batch, state_dim]\n",
    "        cov = torch.diag_embed(var) # [batch, state_dim, state_dim]\n",
    "        m = MultivariateNormal(mu, cov)\n",
    "        ll = m.log_prob(label) # [batch]\n",
    "        return ll\n",
    "\n",
    "    def predict(self, s, a):\n",
    "        # convert to torch format\n",
    "        s = CUDA(torch.tensor(s).float())\n",
    "        a = CUDA(torch.tensor(a).float())\n",
    "        inputs = torch.cat((s, a), axis=1)\n",
    "        state_next = self.model(inputs)[0].cpu().detach().numpy()\n",
    "        return state_next\n",
    "    \n",
    "    def add_data_point(self, data):\n",
    "        # data format: [task_idx, state, action, next_state-state]\n",
    "        self.dataset.append(data)\n",
    "        \n",
    "    def reset_dataset(self, new_dataset = None):\n",
    "        # dataset format: list of [task_idx, state, action, next_state-state]\n",
    "        if new_dataset is not None:\n",
    "            self.dataset = new_dataset\n",
    "        else:\n",
    "            self.dataset = []\n",
    "            \n",
    "    def make_dataset(self, dataset, make_test_set = False):\n",
    "        # dataset format: list of [task_idx, state, action, next_state-state]\n",
    "        num_data = len(dataset)\n",
    "        data_list = []\n",
    "        for data in dataset:\n",
    "            s = data[1] # state\n",
    "            a = data[2] # action\n",
    "            label = data[3] # here label means the next state [state dim]\n",
    "            data = np.concatenate((s, a), axis=0) # [state dim + action dim]\n",
    "            data_torch = CUDA(torch.Tensor(data))\n",
    "            label_torch = CUDA(torch.Tensor(label))\n",
    "            data_list.append([data_torch, label_torch])\n",
    "            \n",
    "        if make_test_set:\n",
    "            indices = list(range(num_data))\n",
    "            split = int(np.floor(self.validation_ratio * num_data))\n",
    "            np.random.shuffle(indices)\n",
    "            train_idx, test_idx = indices[split:], indices[:split]\n",
    "            train_set = [data_list[idx] for idx in train_idx]\n",
    "            test_set = [data_list[idx] for idx in test_idx]\n",
    "            train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=self.batch_size)\n",
    "            test_loader = torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=self.batch_size)\n",
    "        else:\n",
    "            train_loader = torch.utils.data.DataLoader(data_list, shuffle=True, batch_size=self.batch_size)\n",
    "            test_loader = None\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def fit(self, dataset=None):\n",
    "        if dataset is not None:\n",
    "            train_loader, test_loader = self.make_dataset(dataset, make_test_set=self.validation_flag)\n",
    "        else: # use its own accumulated data\n",
    "            train_loader, test_loader = self.make_dataset(self.dataset, make_test_set=self.validation_flag)\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            loss_this_epoch = []\n",
    "            for datas, labels in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(datas)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                loss_this_epoch.append(loss.item())\n",
    "            \n",
    "            if self.save_model_flag:\n",
    "                torch.save(self.model, self.save_model_path)\n",
    "                \n",
    "            if self.validation_flag and (epoch+1) % self.validate_freq == 0:\n",
    "                loss_test, mse_test = self.validate_model(test_loader)\n",
    "                loss_train, mse_train = self.validate_model(train_loader)\n",
    "                print(f\"training epoch [{epoch}/{self.n_epochs}],loss train: {loss_train:.4f}/{mse_train:.4f}, loss test  {loss_test:.4f}/{mse_test:.4f}\")\n",
    "\n",
    "        return np.mean(loss_this_epoch)\n",
    "\n",
    "    def validate_model(self, testloader):\n",
    "        loss_list = []\n",
    "        mse_list = []\n",
    "        for datas, labels in testloader:\n",
    "            outputs = self.model(datas)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            mse_loss = self.mse(outputs[0], labels)\n",
    "            loss_list.append(loss.item())\n",
    "            mse_list.append(mse_loss.item())\n",
    "        return np.mean(loss_list), np.mean(mse_list)\n",
    "    \n",
    "    def split_train_validation_old(self):\n",
    "        num_data = len(self.data)\n",
    "        # use validation\n",
    "        if self.validation_flag:\n",
    "            indices = list(range(num_data))\n",
    "            split = int(np.floor(self.validation_ratio * num_data))\n",
    "            np.random.shuffle(indices)\n",
    "            train_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "            train_set = [[self.data[idx], self.label[idx]] for idx in train_idx]\n",
    "            test_set = [[self.data[idx], self.label[idx]] for idx in test_idx]\n",
    "\n",
    "            train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=self.batch_size)\n",
    "            test_loader = torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=self.batch_size)\n",
    "        else:\n",
    "            train_set = [[self.data[idx], self.label[idx]] for idx in range(num_data)]\n",
    "            train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=self.batch_size)\n",
    "            test_loader = None\n",
    "            \n",
    "        return train_loader, test_loader\n",
    "\n",
    "    def add_data_point_old(self, data):\n",
    "        s = data[1]\n",
    "        a = data[2]\n",
    "        label = data[3][None] # here label means the next state\n",
    "        data = np.concatenate((s, a), axis=0)[None]\n",
    "\n",
    "        # add new data point to data buffer\n",
    "        if self.data is None:\n",
    "            self.data = CUDA(torch.Tensor(data))\n",
    "            self.label = CUDA(torch.Tensor(label))\n",
    "        else:\n",
    "            self.data = torch.cat((self.data, CUDA(torch.tensor(data).float())), dim=0)\n",
    "            self.label = torch.cat((self.label, CUDA(torch.tensor(label).float())), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.FloatTensor([[0,1],[1,0],[1,0]])\n",
    "var = torch.FloatTensor([[1,1],[1,1],[5,5]])\n",
    "cov = torch.diag_embed(var)\n",
    "print(mean.shape, var.shape)\n",
    "m = torch.distributions.MultivariateNormal(mean, cov)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
