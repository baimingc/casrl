{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@Author:\n",
    "@Email: \n",
    "@Date: 2020-04-01 01:26:48\n",
    "@LastEditTime: 2020-04-16 00:15:58\n",
    "@Description: \n",
    "'''\n",
    "\n",
    "import time, datetime\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import yaml\n",
    "\n",
    "import gym\n",
    "sys.path.append('./envs/cartpole-envs')\n",
    "sys.path.append('./envs/highway-env')\n",
    "import cartpole_envs\n",
    "#import highway_env\n",
    "\n",
    "from utils import plot_reward, plot_index, dumb_reward_plot\n",
    "from mpc.mpc_cp import MPC\n",
    "\n",
    "# all models\n",
    "# from dpgpmm.DPGPMM import DPGPMM\n",
    "from baselines.SingleGP import SingleGP\n",
    "# from baselines.SingleSparseGP import SingleSparseGP\n",
    "# from baselines.NN import NN\n",
    "\n",
    "\n",
    "def prepare_dynamics(gym_config):\n",
    "    dynamics_name = gym_config['dynamics_name']\n",
    "    seed = gym_config['seed']\n",
    "    dynamics_set = []\n",
    "    for i in range(len(dynamics_name)):\n",
    "        env = gym.make(dynamics_name[i])\n",
    "        # env.seed(seed)\n",
    "        dynamics_set.append(gym.make(dynamics_name[i]))\n",
    "    \n",
    "    # use pre-defined env sequence\n",
    "    task = [dynamics_set[i] for i in gym_config['task_dynamics_list']]\n",
    "    return task\n",
    "\n",
    "\n",
    "def load_config(config_path=\"config.yml\"):\n",
    "    if os.path.isfile(config_path):\n",
    "        f = open(config_path)\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)\n",
    "    else:\n",
    "        raise Exception(\"Configuration file is not found in the path: \"+config_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-02 15:14:09.848 | INFO     | __main__:<module>:17 - Using model: SingleGP\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dynamic model configuration\n",
    "# config = load_config('config_DPGP_MBRL.yml')\n",
    "config = load_config('./config/config_swingup.yml')\n",
    "dpgp_config = config['DPGP_config']\n",
    "gp_config = config['SingleGP_config']\n",
    "sparse_gp_config = config['SingleSparseGP_config']\n",
    "nn_config = config['NN_config']\n",
    "mpc_config = config['mpc_config']\n",
    "gym_config = config['gym_config']\n",
    "render = gym_config['render']\n",
    "\n",
    "# initialize the mixture model\n",
    "# model = DPGPMM(dpgp_config=dpgp_config)\n",
    "# model = SingleSparseGP(sparse_gp_config=sparse_gp_config)\n",
    "model = SingleGP(gp_config=gp_config)\n",
    "# model = NN(NN_config=nn_config)\n",
    "logger.info('Using model: {}', model.name)\n",
    "\n",
    "# initial MPC controller\n",
    "mpc_controller = MPC(mpc_config=mpc_config)\n",
    "\n",
    "# prepare task\n",
    "# the task is solved, if each dynamic is solved\n",
    "task = prepare_dynamics(gym_config)\n",
    "\n",
    "\"\"\"start DPGP-MBRL\"\"\"\n",
    "data_buffer = []\n",
    "label_list = []\n",
    "subtask_list = []\n",
    "subtask_reward = []\n",
    "subtask_succ_count = [0]\n",
    "comp_trainable = [1]\n",
    "task_reward = []\n",
    "trainable = True\n",
    "task_solved = False\n",
    "subtask_solved = [False, False, False, False]\n",
    "total_count = 0\n",
    "task_epi = 0\n",
    "log = []\n",
    "log_name = None\n",
    "\n",
    "# if model.name == 'NN':\n",
    "#     pretrain_episodes = 10\n",
    "#     print('pretrain~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "#     for task_idx in range(len(task)):\n",
    "#         env = task[task_idx]\n",
    "#         # data collection\n",
    "#         for epi in range(pretrain_episodes):\n",
    "#             obs = env.reset()\n",
    "#             done = False\n",
    "#             mpc_controller.reset()\n",
    "#             while not done:\n",
    "#                 action = env.action_space.sample()\n",
    "#                 obs_next, reward, done, state_next = env.step(action)\n",
    "#                 model.data_process([0, obs, action, obs_next - obs])\n",
    "#                 obs = copy.deepcopy(obs_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtask:  0 , epi:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-74c30d13f766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# print('mpc: {}, env: {}, model: {}'.format(start_2-start_1, start_3-start_2, start_4-start_3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pole_mass: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pole_length: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'step: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc_reward: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'violation_rate: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "while task_epi < 30:\n",
    "    task_epi += 1\n",
    "    time_task_0 = time.time()\n",
    "    if total_count == 0:\n",
    "        # for the first step, add one data pair with random policy as initialization\n",
    "        state = task[0].reset()\n",
    "        action = task[0].action_space.sample()\n",
    "        state_next, reward, done, info = task[0].step(action)\n",
    "        model.fit(data=[0, state, action, state_next-state])\n",
    "        label_list.append(0)\n",
    "\n",
    "    # for other steps, run DPGP MBRL\n",
    "    # Different sub-tasks share the same action space\n",
    "    # Note that the subtask_index is unknown to the model, it's for debugging\n",
    "    task_r = 0\n",
    "    for subtask_index in range(len(task)):\n",
    "        m_p = np.random.uniform(0.3,0.7)\n",
    "        l = np.random.uniform(0.3,0.7)\n",
    "#         l = np.random.uniform(0.2,1.0)\n",
    "        task[subtask_index].unwrapped.m_p = m_p\n",
    "        task[subtask_index].unwrapped.l = l\n",
    "        \n",
    "        for epi in range(1): # each subtask contains a fixed number of episode\n",
    "            O, A, R, acc_reward, done, V = [], [], [], 0, False, []\n",
    "\n",
    "            print('subtask: ', subtask_index, ', epi: ', epi)\n",
    "            time_subtask_0 = time.time()\n",
    "\n",
    "            state = task[subtask_index].reset()\n",
    "            O.append(state)\n",
    "            # reset the controller at the beginning of each new dynamic\n",
    "            mpc_controller.reset()\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    task[subtask_index].render()\n",
    "\n",
    "                total_count += 1\n",
    "                label_list.append(subtask_index)\n",
    "\n",
    "                # MPC policy\n",
    "                start_1 = time.time()\n",
    "                action = np.array([mpc_controller.act(task=task[subtask_index], model=model, state=state)])\n",
    "                start_2 = time.time()\n",
    "\n",
    "                # Random Policy\n",
    "                # action = task[subtask_index].action_space.sample()\n",
    "\n",
    "                # interact with env\n",
    "                state_next, reward, done, violation = task[subtask_index].step(action)\n",
    "                acc_reward += reward\n",
    "\n",
    "#                 print('action ', action)\n",
    "#                 print('reward: %.4f' % reward)\n",
    "\n",
    "                A.append(action)\n",
    "                O.append(state_next)\n",
    "                R.append(reward)\n",
    "\n",
    "                # logger.info('acc_reward : {}', acc_reward)\n",
    "                start_3 = time.time()\n",
    "\n",
    "                # train the model\n",
    "                # when reach some kind of metric, stop training, only inference\n",
    "\n",
    "                model.fit(data=[subtask_index, state, action, state_next - state])\n",
    "\n",
    "                state = copy.deepcopy(state_next)\n",
    "                start_4 = time.time()\n",
    "\n",
    "                # print('mpc: {}, env: {}, model: {}'.format(start_2-start_1, start_3-start_2, start_4-start_3))\n",
    "            print('pole_mass: ', m_p, 'pole_length: ', l,'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            env.close()\n",
    "\n",
    "            if done:\n",
    "                samples = {\n",
    "                    \"obs\": np.array(O),\n",
    "                    \"actions\": np.array(A),\n",
    "                    \"rewards\": np.array(R),\n",
    "                    \"reward_sum\": acc_reward,\n",
    "                    \"violations\": np.array(V)\n",
    "                }\n",
    "                log.append(samples)\n",
    "                if log_name is None:\n",
    "                    log_name = datetime.datetime.now()\n",
    "                path = './misc/log/gp_robust_' + log_name.strftime(\"%d-%H-%M\") + '.npy'\n",
    "                np.save(path, log, allow_pickle=True)\n",
    "                dumb_reward_plot(path)\n",
    "#                 if done:\n",
    "#                     samples = {\n",
    "#                         \"obs\": np.array(O),\n",
    "#                         \"actions\": np.array(A),\n",
    "#                         \"rewards\": np.array(R),\n",
    "#                         \"reward_sum\": acc_reward,\n",
    "#                     }\n",
    "#                     log.append(samples)\n",
    "#                     if log_name is None:\n",
    "#                         log_name = datetime.datetime.now()\n",
    "#                     path = './misc/log/CartPole-GP-'+ log_name.strftime(\"%d-%H-%M\") + '.npy'\n",
    "#                     np.save(path, log, allow_pickle=True)\n",
    "#                     dumb_reward_plot(path)\n",
    "\n",
    "#                     print('-------------------------------------------------')\n",
    "#                     print('pole_mass', m_p, 'pole_length', l, 'Episode finished, time: ', time.time()-time_subtask_0, ' with acc_reward: ', acc_reward,\n",
    "#                           ' with final reward: ', reward)\n",
    "#                     print('-------------------------------------------------')\n",
    "#                     subtask_list.append(subtask_index)\n",
    "#                     subtask_reward.append(acc_reward)\n",
    "#                     task_r += acc_reward\n",
    "#                     if not model.name == 'DPGPMM':\n",
    "#                         if len(subtask_succ_count) < subtask_index + 1:\n",
    "#                             subtask_succ_count.append(0)\n",
    "#                     if acc_reward >= 170:\n",
    "#                         subtask_solved[subtask_index] = True\n",
    "#                         print('-------------------------------------------------')\n",
    "#                         print('Episode finished: Success!!!!, time: ', time.time()-time_subtask_0)\n",
    "#                         print('-------------------------------------------------')\n",
    "#                         subtask_list.append(subtask_index)\n",
    "#                         subtask_reward.append(acc_reward)\n",
    "#                         task_r += acc_reward\n",
    "#                         # record succ rate\n",
    "#                         if model.name == 'DPGPMM':\n",
    "#                             subtask_succ_count[model.DP_mix.assigns[len(model.DP_mix.data) - 1]] += 1\n",
    "#                         else:\n",
    "#                             if len(subtask_succ_count) < subtask_index + 1:\n",
    "#                                 subtask_succ_count.append(1)\n",
    "#                             else:\n",
    "#                                 subtask_succ_count[subtask_index] += 1\n",
    "\n",
    "#             if model.name == 'DPGPMM':\n",
    "#                 print('subtask_succ_count: ', subtask_succ_count)\n",
    "#                 # todo: check the training termination criterion right or not\n",
    "#                 for i in range(len(subtask_succ_count)):\n",
    "#                     if subtask_succ_count[i] >= 10:\n",
    "#                         comp_trainable[i] = 0\n",
    "#             else:\n",
    "#                 print('subtask_succ_count: ', subtask_succ_count)\n",
    "#                 # todo: check the training termination criterion right or not\n",
    "#                 all_solve = 0\n",
    "#                 for i in range(len(subtask_succ_count)):\n",
    "#                     if subtask_succ_count[i] >= 10:\n",
    "#                         all_solve += 1\n",
    "#                 if all_solve == 4:\n",
    "#                     trainable = False\n",
    "#             if render:\n",
    "#                 task[subtask_index].close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
