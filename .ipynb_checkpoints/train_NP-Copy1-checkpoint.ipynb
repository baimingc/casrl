{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import yaml\n",
    "from utils import dumb_reward_plot\n",
    "import gym\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "sys.path.append('./envs/cartpole-envs')\n",
    "sys.path.append('./')\n",
    "import cartpole_envs\n",
    "\n",
    "from utils import plot_reward, plot_index\n",
    "from mpc.mpc_cp import MPC\n",
    "from baselines.NP_epi import NP\n",
    "\n",
    "def prepare_dynamics(gym_config):\n",
    "    dynamics_name = gym_config['dynamics_name']\n",
    "    seed = gym_config['seed']\n",
    "    dynamics_set = []\n",
    "    for i in range(len(dynamics_name)):\n",
    "        dynamics_set.append(gym.make(dynamics_name[i]))\n",
    "    task = [dynamics_set[i] for i in gym_config['task_dynamics_list']]\n",
    "    return task\n",
    "\n",
    "def load_config(config_path=\"config.yml\"):\n",
    "    if os.path.isfile(config_path):\n",
    "        f = open(config_path)\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)\n",
    "    else:\n",
    "        raise Exception(\"Configuration file is not found in the path: \"+config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-07 17:12:19.689 | INFO     | __main__:<module>:9 - Using model: NP\n"
     ]
    }
   ],
   "source": [
    "# config = load_config('config/config_cpstable_np.yml')\n",
    "config = load_config('config/config_swingup_robust.yml')\n",
    "mpc_config = config['mpc_config']\n",
    "gym_config = config['gym_config']\n",
    "render = gym_config['render']\n",
    "np_config = config['NP_config']\n",
    "\n",
    "model = NP(NP_config=np_config)\n",
    "logger.info('Using model: {}', model.name)\n",
    "\n",
    "mpc_controller = MPC(mpc_config=mpc_config)\n",
    "\n",
    "# prepare task\n",
    "task = prepare_dynamics(gym_config)\n",
    "# print(gym_config)\n",
    "\n",
    "\"\"\"start DPGP-MBRL\"\"\"\n",
    "data_buffer = []\n",
    "label_list = []\n",
    "subtask_list = []\n",
    "subtask_reward = []\n",
    "subtask_succ_count = [0]\n",
    "comp_trainable = [1]\n",
    "task_reward = []\n",
    "trainable = True\n",
    "task_solved = False\n",
    "subtask_solved = [False, False, False, False]\n",
    "total_count = 0\n",
    "task_epi = 0\n",
    "log_name = None\n",
    "\n",
    "total_tasks = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NP pretrain\"\"\"\n",
    "m_p_list = [0.3, 0.7]\n",
    "l_list = [0.3, 0.7]\n",
    "pretrain_episodes = 1\n",
    "for task_idx in range(10):\n",
    "    env = task[0]\n",
    "#     m_p = m_p_list[np.random.randint(2)]\n",
    "#     l = l_list[np.random.randint(2)]\n",
    "    m_p = np.random.uniform(0.3, 0.7)\n",
    "    l = np.random.uniform(0.3, 0.7)\n",
    "    env.unwrapped.m_p = m_p\n",
    "    env.unwrapped.l = l\n",
    "    for epi in range(pretrain_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        mpc_controller.reset()\n",
    "        i = 0\n",
    "        while not done:\n",
    "            i += 1\n",
    "            action = env.action_space.sample()\n",
    "            obs_next, reward, done, _ = env.step(action)\n",
    "            model.data_process([0, obs, action, obs_next - obs])\n",
    "#             if i > 3:\n",
    "#                 model.train()\n",
    "            obs = obs_next\n",
    "    model.reset()\n",
    "    model.train()\n",
    "# torch.save(model.model.state_dict(), './misc/log/model_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = NP(NP_config=np_config)\n",
    "# model2.model.load_state_dict(torch.load( './misc/log/model_test.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pole_mass:  0.6444561713555839 pole_length:  0.40269517328413584 step:  200 acc_reward:  8.058455484034104 violation_rate:  0.0\n",
      "pole_mass:  0.31800205678250876 pole_length:  0.6598690930009012 step:  200 acc_reward:  11.359121118294404 violation_rate:  0.045\n",
      "pole_mass:  0.47425849080156735 pole_length:  0.6000070508708291 step:  200 acc_reward:  47.161480789723186 violation_rate:  0.1\n",
      "pole_mass:  0.337123378654993 pole_length:  0.5750115351113498 step:  200 acc_reward:  23.563826472750723 violation_rate:  0.02\n",
      "pole_mass:  0.5154667488169822 pole_length:  0.692585671241827 step:  200 acc_reward:  54.51951743824596 violation_rate:  0.15\n",
      "pole_mass:  0.4358735061307504 pole_length:  0.3503362747532297 step:  200 acc_reward:  40.193319971282484 violation_rate:  0.055\n",
      "pole_mass:  0.5807264512588632 pole_length:  0.6416040780394315 step:  200 acc_reward:  40.328024240202275 violation_rate:  0.03\n",
      "pole_mass:  0.5552989261499768 pole_length:  0.4565785847959813 step:  200 acc_reward:  51.548831961988306 violation_rate:  0.035\n",
      "pole_mass:  0.347866552148109 pole_length:  0.5840232733305635 step:  200 acc_reward:  47.528151012667436 violation_rate:  0.075\n",
      "pole_mass:  0.4850373482927226 pole_length:  0.48267003928873287 step:  200 acc_reward:  74.6937666941175 violation_rate:  0.025\n",
      "pole_mass:  0.6849582257653961 pole_length:  0.39866142389246473 step:  200 acc_reward:  14.09749858554758 violation_rate:  0.02\n",
      "pole_mass:  0.32365452761579216 pole_length:  0.5088987139772161 step:  200 acc_reward:  33.39771727124371 violation_rate:  0.0\n",
      "pole_mass:  0.6423463877131493 pole_length:  0.44653802712055835 step:  200 acc_reward:  15.680968161779681 violation_rate:  0.0\n",
      "pole_mass:  0.4780724672144655 pole_length:  0.4289914238654794 step:  200 acc_reward:  62.496976398794374 violation_rate:  0.0\n",
      "pole_mass:  0.5517204695967648 pole_length:  0.422274360520715 step:  200 acc_reward:  79.65526020809706 violation_rate:  0.0\n",
      "pole_mass:  0.5028981757889759 pole_length:  0.5586646664528061 step:  200 acc_reward:  76.32595688778086 violation_rate:  0.0\n",
      "pole_mass:  0.5061688070641639 pole_length:  0.30646887621450275 step:  200 acc_reward:  52.64873388287011 violation_rate:  0.095\n",
      "pole_mass:  0.3878815240723655 pole_length:  0.634250543278804 step:  200 acc_reward:  59.00384039494056 violation_rate:  0.025\n",
      "pole_mass:  0.3789656494765772 pole_length:  0.5806670536482033 step:  200 acc_reward:  63.2293624925555 violation_rate:  0.01\n",
      "pole_mass:  0.33954564252544656 pole_length:  0.3010747030865948 step:  200 acc_reward:  29.914473073983128 violation_rate:  0.05\n",
      "pole_mass:  0.5515032506228044 pole_length:  0.5193725603726436 step:  200 acc_reward:  56.20196651574563 violation_rate:  0.0\n",
      "pole_mass:  0.5947633196189597 pole_length:  0.4968261935180121 step:  200 acc_reward:  42.726892214617834 violation_rate:  0.0\n",
      "pole_mass:  0.6640339752905704 pole_length:  0.3219583111432574 step:  200 acc_reward:  53.24878094453937 violation_rate:  0.0\n",
      "pole_mass:  0.41462787653967803 pole_length:  0.4249398931475965 step:  200 acc_reward:  88.41627868401616 violation_rate:  0.025\n",
      "pole_mass:  0.3259484561619193 pole_length:  0.46917775578543397 step:  200 acc_reward:  87.82145518795363 violation_rate:  0.0\n",
      "pole_mass:  0.6613332346882393 pole_length:  0.38285435655857963 step:  200 acc_reward:  57.250730018442844 violation_rate:  0.0\n",
      "pole_mass:  0.41495883645599574 pole_length:  0.6524232347096584 step:  200 acc_reward:  28.041962881140204 violation_rate:  0.0\n",
      "pole_mass:  0.6928689326423773 pole_length:  0.4359062193251771 step:  200 acc_reward:  84.24948850645816 violation_rate:  0.0\n",
      "pole_mass:  0.6510782446563573 pole_length:  0.6952758388539229 step:  200 acc_reward:  90.9805255818486 violation_rate:  0.0\n",
      "pole_mass:  0.40333870499767066 pole_length:  0.5232643864414215 step:  200 acc_reward:  71.07709760709379 violation_rate:  0.0\n",
      "pole_mass:  0.572260841636484 pole_length:  0.3124113355163719 step:  200 acc_reward:  43.457825623948374 violation_rate:  0.02\n",
      "pole_mass:  0.58997656853034 pole_length:  0.5569538598975248 step:  200 acc_reward:  63.51445179227405 violation_rate:  0.0\n",
      "pole_mass:  0.5923273614277564 pole_length:  0.4625322082352268 step:  200 acc_reward:  135.4933076006163 violation_rate:  0.0\n",
      "pole_mass:  0.559546638270604 pole_length:  0.38978473871635355 step:  200 acc_reward:  136.1473974558996 violation_rate:  0.015\n",
      "pole_mass:  0.6918006666804316 pole_length:  0.5822311949016801 step:  200 acc_reward:  77.98278230910518 violation_rate:  0.025\n",
      "pole_mass:  0.49015124934809917 pole_length:  0.47743531639227443 step:  200 acc_reward:  93.47328599003835 violation_rate:  0.0\n",
      "pole_mass:  0.5483534291870586 pole_length:  0.3757952185192319 step:  200 acc_reward:  77.50375344316272 violation_rate:  0.01\n",
      "pole_mass:  0.5597354441076858 pole_length:  0.6970638390860288 step:  200 acc_reward:  53.61355763435738 violation_rate:  0.07\n",
      "pole_mass:  0.39792319863749037 pole_length:  0.4184746708914804 step:  200 acc_reward:  73.8519675193645 violation_rate:  0.0\n",
      "pole_mass:  0.5133309371739264 pole_length:  0.4004757700001749 step:  200 acc_reward:  67.17533362867157 violation_rate:  0.015\n",
      "pole_mass:  0.6111942346502606 pole_length:  0.5419090150404748 step:  200 acc_reward:  54.43073814198517 violation_rate:  0.015\n",
      "pole_mass:  0.6755607885292316 pole_length:  0.6407217646665554 step:  200 acc_reward:  54.23276028475765 violation_rate:  0.085\n",
      "pole_mass:  0.4624599713317163 pole_length:  0.6847466589689277 step:  200 acc_reward:  75.29988478459292 violation_rate:  0.06\n",
      "pole_mass:  0.5231606439910006 pole_length:  0.571116126181481 step:  200 acc_reward:  89.18964017806857 violation_rate:  0.02\n",
      "pole_mass:  0.5002261757300789 pole_length:  0.38899332260693775 step:  200 acc_reward:  69.77849950900985 violation_rate:  0.0\n",
      "pole_mass:  0.6511643902056397 pole_length:  0.38452540662426365 step:  200 acc_reward:  76.52191744547311 violation_rate:  0.02\n",
      "pole_mass:  0.5162226734530828 pole_length:  0.33062276734172397 step:  200 acc_reward:  59.46140403239899 violation_rate:  0.015\n",
      "pole_mass:  0.6189981301823084 pole_length:  0.596723337924008 step:  200 acc_reward:  83.42982424230038 violation_rate:  0.01\n",
      "pole_mass:  0.4309733627497314 pole_length:  0.6329952152126394 step:  200 acc_reward:  66.38036499901172 violation_rate:  0.025\n",
      "pole_mass:  0.3848988144973005 pole_length:  0.5427046605670006 step:  200 acc_reward:  99.98279782367088 violation_rate:  0.0\n",
      "pole_mass:  0.4734754297580801 pole_length:  0.5166960735611076 step:  200 acc_reward:  94.84691963984446 violation_rate:  0.0\n",
      "pole_mass:  0.33073245903149673 pole_length:  0.3553080296346854 step:  200 acc_reward:  96.12592992429434 violation_rate:  0.02\n",
      "pole_mass:  0.5033793271660337 pole_length:  0.4249736551672036 step:  200 acc_reward:  100.66528408975078 violation_rate:  0.0\n",
      "pole_mass:  0.3975204306304836 pole_length:  0.3775497995422488 step:  200 acc_reward:  88.53228420877859 violation_rate:  0.0\n",
      "pole_mass:  0.5980076954300082 pole_length:  0.3839411913662069 step:  200 acc_reward:  67.80269087579927 violation_rate:  0.02\n",
      "pole_mass:  0.48481287611626034 pole_length:  0.33639245898469256 step:  200 acc_reward:  69.7646484980902 violation_rate:  0.0\n",
      "pole_mass:  0.5221115634909306 pole_length:  0.3547585544312408 step:  200 acc_reward:  74.23127769835041 violation_rate:  0.0\n",
      "pole_mass:  0.37773343686397776 pole_length:  0.30983790168134057 step:  200 acc_reward:  24.00422954200951 violation_rate:  0.065\n",
      "pole_mass:  0.5835893078717589 pole_length:  0.6772075330909862 step:  200 acc_reward:  71.12037554343533 violation_rate:  0.0\n",
      "pole_mass:  0.35828777100385767 pole_length:  0.49516430782189447 step:  200 acc_reward:  105.10471498129085 violation_rate:  0.0\n",
      "pole_mass:  0.6890024966205625 pole_length:  0.44079653231184196 step:  200 acc_reward:  64.06057458204798 violation_rate:  0.0\n",
      "pole_mass:  0.3537171112381623 pole_length:  0.5231067135913696 step:  200 acc_reward:  111.03029040251612 violation_rate:  0.0\n",
      "pole_mass:  0.33322169525265466 pole_length:  0.4132441267680979 step:  200 acc_reward:  119.37353936739909 violation_rate:  0.0\n",
      "pole_mass:  0.5549068345044001 pole_length:  0.3201158223378689 step:  200 acc_reward:  39.784313808591214 violation_rate:  0.015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pole_mass:  0.5800335595764814 pole_length:  0.5020465471764977 step:  200 acc_reward:  65.41007667772541 violation_rate:  0.0\n",
      "pole_mass:  0.6041062787239677 pole_length:  0.6868801746074387 step:  200 acc_reward:  59.389238257135055 violation_rate:  0.03\n",
      "pole_mass:  0.6151888158356036 pole_length:  0.6564979160235828 step:  200 acc_reward:  92.97368769140677 violation_rate:  0.0\n",
      "pole_mass:  0.5272346600437731 pole_length:  0.5893082526104159 step:  200 acc_reward:  120.61682577297138 violation_rate:  0.0\n",
      "pole_mass:  0.42324229070632813 pole_length:  0.5632904137917669 step:  200 acc_reward:  95.36425761809949 violation_rate:  0.0\n",
      "pole_mass:  0.6549338359100891 pole_length:  0.4527207399563733 step:  200 acc_reward:  144.5238505975788 violation_rate:  0.005\n",
      "pole_mass:  0.4887750503850905 pole_length:  0.4154072596469818 step:  200 acc_reward:  96.23959688611477 violation_rate:  0.0\n",
      "pole_mass:  0.42528402562880485 pole_length:  0.3598450164004036 step:  200 acc_reward:  54.93216196767582 violation_rate:  0.0\n",
      "pole_mass:  0.4781693746698058 pole_length:  0.3853891269336279 step:  200 acc_reward:  95.99843871455948 violation_rate:  0.0\n",
      "pole_mass:  0.40165059471670533 pole_length:  0.5326954691267669 step:  200 acc_reward:  101.30558496387906 violation_rate:  0.01\n",
      "pole_mass:  0.6279009053123639 pole_length:  0.5258723179476036 step:  200 acc_reward:  82.95195909669236 violation_rate:  0.0\n",
      "pole_mass:  0.3126903828338538 pole_length:  0.3989120300969271 step:  200 acc_reward:  85.98466786458907 violation_rate:  0.0\n",
      "pole_mass:  0.409225189334599 pole_length:  0.37679930855151267 step:  200 acc_reward:  155.32263765790165 violation_rate:  0.005\n",
      "pole_mass:  0.6091039876866675 pole_length:  0.5669055342584762 step:  200 acc_reward:  129.39415849101476 violation_rate:  0.005\n",
      "pole_mass:  0.4573150551999996 pole_length:  0.42662882921673995 step:  200 acc_reward:  117.83512461388518 violation_rate:  0.015\n",
      "pole_mass:  0.5748851066525782 pole_length:  0.5690472907804238 step:  200 acc_reward:  83.81351084280851 violation_rate:  0.0\n",
      "pole_mass:  0.5842974283247842 pole_length:  0.5816472371731722 step:  200 acc_reward:  122.47639821745533 violation_rate:  0.0\n",
      "pole_mass:  0.5668071533334584 pole_length:  0.482375079272958 step:  200 acc_reward:  118.71725720058787 violation_rate:  0.0\n",
      "pole_mass:  0.38175838090454556 pole_length:  0.3814407623260493 step:  200 acc_reward:  57.20313654524183 violation_rate:  0.13\n",
      "pole_mass:  0.47778024793043106 pole_length:  0.393729168729414 step:  200 acc_reward:  77.14637934638694 violation_rate:  0.0\n",
      "pole_mass:  0.3063050617647303 pole_length:  0.5218466574665834 step:  200 acc_reward:  177.45491493634088 violation_rate:  0.0\n",
      "pole_mass:  0.6319366611457997 pole_length:  0.30461326536657235 step:  94 acc_reward:  24.77752055167038 violation_rate:  0.07446808510638298\n",
      "pole_mass:  0.3048860534250533 pole_length:  0.5154495022868266 step:  200 acc_reward:  87.46725964725026 violation_rate:  0.0\n",
      "pole_mass:  0.4862384327303047 pole_length:  0.4992489716131142 step:  200 acc_reward:  121.47904849052003 violation_rate:  0.0\n",
      "pole_mass:  0.5864953539331836 pole_length:  0.5342718516612235 step:  200 acc_reward:  77.40404803080145 violation_rate:  0.0\n",
      "pole_mass:  0.4111266822928431 pole_length:  0.663281769447942 step:  200 acc_reward:  63.492968173244684 violation_rate:  0.0\n",
      "pole_mass:  0.46542232634047065 pole_length:  0.5981917280279974 step:  200 acc_reward:  58.216263931524004 violation_rate:  0.0\n",
      "pole_mass:  0.36836456450995336 pole_length:  0.5031260036662231 step:  200 acc_reward:  84.69215304647473 violation_rate:  0.0\n",
      "pole_mass:  0.5568584273798887 pole_length:  0.6654387984201356 step:  200 acc_reward:  58.5310552394356 violation_rate:  0.04\n",
      "pole_mass:  0.6088921428843697 pole_length:  0.619472101720329 step:  200 acc_reward:  106.91855637625608 violation_rate:  0.0\n",
      "pole_mass:  0.40389006569554553 pole_length:  0.5781245883952759 step:  200 acc_reward:  129.13879703212578 violation_rate:  0.015\n",
      "pole_mass:  0.5733952595271963 pole_length:  0.43053704867294007 step:  200 acc_reward:  114.5546659695891 violation_rate:  0.005\n",
      "pole_mass:  0.6277470293695175 pole_length:  0.6110246143829781 step:  200 acc_reward:  87.77445324386285 violation_rate:  0.005\n",
      "pole_mass:  0.33538217455402974 pole_length:  0.6056468422424526 step:  200 acc_reward:  91.83092994025165 violation_rate:  0.04\n",
      "pole_mass:  0.56192337657638 pole_length:  0.47285686389633597 step:  200 acc_reward:  88.12053485959723 violation_rate:  0.0\n",
      "pole_mass:  0.6552349268825403 pole_length:  0.6166748759486778 step:  200 acc_reward:  91.6393980180294 violation_rate:  0.02\n",
      "pole_mass:  0.5027486642075687 pole_length:  0.4183090127059523 step:  200 acc_reward:  164.34073615017536 violation_rate:  0.0\n",
      "pole_mass:  0.4835091290677759 pole_length:  0.5636114295823682 step:  200 acc_reward:  78.06038619466376 violation_rate:  0.005\n",
      "pole_mass:  0.5027217873300757 pole_length:  0.3082987852853411 step:  200 acc_reward:  56.18120320784876 violation_rate:  0.05\n",
      "pole_mass:  0.565082189087746 pole_length:  0.4432973749857984 step:  200 acc_reward:  100.10483791464054 violation_rate:  0.0\n",
      "pole_mass:  0.6105942241922524 pole_length:  0.3332984450972864 step:  200 acc_reward:  102.41178057204036 violation_rate:  0.02\n",
      "pole_mass:  0.576759503565024 pole_length:  0.5900810351974436 step:  200 acc_reward:  60.65191079034814 violation_rate:  0.0\n",
      "pole_mass:  0.44234079994918823 pole_length:  0.6364856761116567 step:  200 acc_reward:  73.24249787591222 violation_rate:  0.015\n",
      "pole_mass:  0.4748470280734694 pole_length:  0.5772875751764284 step:  200 acc_reward:  97.79839220236819 violation_rate:  0.0\n",
      "pole_mass:  0.3942794844198281 pole_length:  0.3646080263264565 step:  200 acc_reward:  81.0107777155713 violation_rate:  0.005\n",
      "pole_mass:  0.38742301524767353 pole_length:  0.5952849208251505 step:  200 acc_reward:  54.38101572089377 violation_rate:  0.0\n",
      "pole_mass:  0.4396182183460035 pole_length:  0.6259546504441837 step:  200 acc_reward:  82.60819929032664 violation_rate:  0.025\n",
      "pole_mass:  0.5846123695060407 pole_length:  0.5738787187818093 step:  200 acc_reward:  116.35874118292881 violation_rate:  0.0\n",
      "pole_mass:  0.401586524560289 pole_length:  0.6852870391750203 step:  200 acc_reward:  88.88132073973236 violation_rate:  0.035\n",
      "pole_mass:  0.5994854253901982 pole_length:  0.6745648576351931 step:  200 acc_reward:  69.03161994012723 violation_rate:  0.01\n",
      "pole_mass:  0.4667752362909635 pole_length:  0.5094931020694651 step:  200 acc_reward:  148.3475686647399 violation_rate:  0.005\n",
      "pole_mass:  0.3520451176395402 pole_length:  0.5317670335248837 step:  200 acc_reward:  155.82938865919002 violation_rate:  0.0\n",
      "pole_mass:  0.6851410759142209 pole_length:  0.37811804060888304 step:  200 acc_reward:  128.50028342351126 violation_rate:  0.045\n",
      "pole_mass:  0.33554698861474663 pole_length:  0.5375423074399003 step:  200 acc_reward:  97.46846527147548 violation_rate:  0.0\n",
      "pole_mass:  0.6539124985423621 pole_length:  0.30160452633082596 step:  200 acc_reward:  46.78866646432975 violation_rate:  0.0\n",
      "pole_mass:  0.3852318314099745 pole_length:  0.41032599819554516 step:  200 acc_reward:  90.23752599172673 violation_rate:  0.0\n",
      "pole_mass:  0.6844495630658796 pole_length:  0.4118289911954729 step:  200 acc_reward:  169.09239168610344 violation_rate:  0.005\n",
      "pole_mass:  0.6898909408999665 pole_length:  0.5839114420862743 step:  200 acc_reward:  66.02589235158639 violation_rate:  0.0\n",
      "pole_mass:  0.3150936945504399 pole_length:  0.40084063721318297 step:  200 acc_reward:  148.11292769652073 violation_rate:  0.025\n",
      "pole_mass:  0.31886127972278994 pole_length:  0.6076789302555594 step:  200 acc_reward:  111.62088313124708 violation_rate:  0.0\n",
      "pole_mass:  0.5485953288961924 pole_length:  0.44894657454527453 step:  200 acc_reward:  82.56974477198875 violation_rate:  0.005\n",
      "pole_mass:  0.6491906014437112 pole_length:  0.5878742578163465 step:  200 acc_reward:  122.59631688452113 violation_rate:  0.0\n",
      "pole_mass:  0.46386343819229625 pole_length:  0.6332232068581746 step:  200 acc_reward:  89.39486818213399 violation_rate:  0.035\n",
      "pole_mass:  0.48385348686522456 pole_length:  0.5870044275841593 step:  200 acc_reward:  92.8705974581861 violation_rate:  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pole_mass:  0.312015104407229 pole_length:  0.6374390839099946 step:  200 acc_reward:  72.19585973887355 violation_rate:  0.0\n",
      "pole_mass:  0.4909175121010897 pole_length:  0.32761268960583545 step:  200 acc_reward:  70.4102803603366 violation_rate:  0.06\n",
      "pole_mass:  0.3918629794725008 pole_length:  0.5799266661756931 step:  200 acc_reward:  100.27358239454567 violation_rate:  0.01\n",
      "pole_mass:  0.471729866084706 pole_length:  0.3470245236243288 step:  200 acc_reward:  64.06582107531341 violation_rate:  0.02\n",
      "pole_mass:  0.6907457526358756 pole_length:  0.6178923808969148 step:  200 acc_reward:  55.60452347259415 violation_rate:  0.0\n",
      "pole_mass:  0.3673137639417971 pole_length:  0.574326584135527 step:  200 acc_reward:  62.49439875166003 violation_rate:  0.015\n",
      "pole_mass:  0.48069695315656924 pole_length:  0.48056890257479484 step:  200 acc_reward:  176.23399591105368 violation_rate:  0.0\n",
      "pole_mass:  0.3570197677734288 pole_length:  0.6937693379205644 step:  200 acc_reward:  74.2125851698579 violation_rate:  0.03\n",
      "pole_mass:  0.33760520375239456 pole_length:  0.6811585704034742 step:  200 acc_reward:  63.75266889471634 violation_rate:  0.01\n",
      "pole_mass:  0.4649626011052621 pole_length:  0.5610847719320782 step:  200 acc_reward:  176.2585412157578 violation_rate:  0.0\n",
      "pole_mass:  0.5629560075926564 pole_length:  0.6699615753228605 step:  200 acc_reward:  125.68286859647792 violation_rate:  0.01\n",
      "pole_mass:  0.5776505000668735 pole_length:  0.6153931740110559 step:  200 acc_reward:  143.09251266428507 violation_rate:  0.01\n",
      "pole_mass:  0.6642557770568025 pole_length:  0.6370788929063839 step:  200 acc_reward:  67.83918504624775 violation_rate:  0.0\n",
      "pole_mass:  0.6043940667226888 pole_length:  0.3413967357097869 step:  200 acc_reward:  65.31875711816926 violation_rate:  0.02\n",
      "pole_mass:  0.6415237988059117 pole_length:  0.3045712739411966 step:  200 acc_reward:  54.61198457987633 violation_rate:  0.0\n",
      "pole_mass:  0.6005625941181914 pole_length:  0.6221767122705563 step:  200 acc_reward:  80.74276538643166 violation_rate:  0.0\n",
      "pole_mass:  0.5679477173399992 pole_length:  0.5733210606738661 step:  200 acc_reward:  104.07482787525423 violation_rate:  0.0\n",
      "pole_mass:  0.42531842768588723 pole_length:  0.5145412122968552 step:  200 acc_reward:  93.13590481115557 violation_rate:  0.0\n",
      "pole_mass:  0.3667067457856591 pole_length:  0.32633538708476134 step:  200 acc_reward:  63.272772720084234 violation_rate:  0.035\n",
      "pole_mass:  0.4565153546067077 pole_length:  0.6240272015858412 step:  200 acc_reward:  137.5035622975922 violation_rate:  0.015\n",
      "pole_mass:  0.33617070247698594 pole_length:  0.5587179765605282 step:  200 acc_reward:  144.08214458929476 violation_rate:  0.0\n",
      "pole_mass:  0.5709812997772925 pole_length:  0.5448490783663477 step:  200 acc_reward:  176.3431500817424 violation_rate:  0.0\n",
      "pole_mass:  0.3247096277077567 pole_length:  0.4958365384249125 step:  200 acc_reward:  95.3868011418735 violation_rate:  0.005\n",
      "pole_mass:  0.610088056654517 pole_length:  0.6075069023995284 step:  200 acc_reward:  73.11213931124641 violation_rate:  0.01\n",
      "pole_mass:  0.4613769576437138 pole_length:  0.4498816804501579 step:  200 acc_reward:  99.67552498411013 violation_rate:  0.0\n",
      "pole_mass:  0.481343025928485 pole_length:  0.6745261724265315 step:  200 acc_reward:  60.79840023943276 violation_rate:  0.005\n",
      "pole_mass:  0.437768507231857 pole_length:  0.4147306439071852 step:  200 acc_reward:  87.78063664358238 violation_rate:  0.0\n",
      "pole_mass:  0.31369113199131266 pole_length:  0.3825494434412844 step:  200 acc_reward:  100.37643084606133 violation_rate:  0.045\n",
      "pole_mass:  0.4202986772028225 pole_length:  0.6834550136614809 step:  200 acc_reward:  68.13161637544634 violation_rate:  0.0\n",
      "pole_mass:  0.6173680430295556 pole_length:  0.42494497367439643 step:  200 acc_reward:  81.60902639025292 violation_rate:  0.0\n",
      "pole_mass:  0.636378196103915 pole_length:  0.37988883074628943 step:  200 acc_reward:  76.35231264168871 violation_rate:  0.0\n",
      "pole_mass:  0.6680956921834322 pole_length:  0.6216078995291838 step:  200 acc_reward:  96.15046705777971 violation_rate:  0.0\n",
      "pole_mass:  0.5384409062394204 pole_length:  0.6523270177441085 step:  200 acc_reward:  84.23694952007473 violation_rate:  0.0\n",
      "pole_mass:  0.6972114642487013 pole_length:  0.3166024706678246 step:  200 acc_reward:  40.79498200658423 violation_rate:  0.0\n",
      "pole_mass:  0.4674938786922387 pole_length:  0.6137569782709777 step:  200 acc_reward:  79.5598649902274 violation_rate:  0.0\n",
      "pole_mass:  0.33376532324750263 pole_length:  0.5107843817149391 step:  200 acc_reward:  154.25337388834657 violation_rate:  0.0\n",
      "pole_mass:  0.4117943588797431 pole_length:  0.34698150111311443 step:  200 acc_reward:  80.52211125240433 violation_rate:  0.0\n",
      "pole_mass:  0.5236651661585421 pole_length:  0.3599914771453865 step:  200 acc_reward:  85.7072592484958 violation_rate:  0.0\n",
      "pole_mass:  0.4969189223900894 pole_length:  0.4485277643895071 step:  200 acc_reward:  82.44299184716586 violation_rate:  0.01\n",
      "pole_mass:  0.4449020671629412 pole_length:  0.5573534510582694 step:  200 acc_reward:  138.67706263518627 violation_rate:  0.0\n",
      "pole_mass:  0.4949827306226753 pole_length:  0.6258783526290568 step:  200 acc_reward:  69.99623583264483 violation_rate:  0.0\n",
      "pole_mass:  0.3911048458572635 pole_length:  0.46297136682686824 step:  200 acc_reward:  100.98318060227635 violation_rate:  0.0\n",
      "pole_mass:  0.5687070011054949 pole_length:  0.43604769821209116 step:  200 acc_reward:  96.7260911190809 violation_rate:  0.0\n",
      "pole_mass:  0.5041028586297784 pole_length:  0.6923689316999829 step:  200 acc_reward:  127.28817260784987 violation_rate:  0.015\n",
      "pole_mass:  0.6227177291377877 pole_length:  0.4233370789410705 step:  200 acc_reward:  76.58678863861813 violation_rate:  0.0\n",
      "pole_mass:  0.6382512884853807 pole_length:  0.42465910928401196 step:  200 acc_reward:  81.73910520621095 violation_rate:  0.0\n",
      "pole_mass:  0.6196199983521755 pole_length:  0.4983768063021011 step:  200 acc_reward:  77.206218273045 violation_rate:  0.0\n",
      "pole_mass:  0.3843929530228745 pole_length:  0.48756807030941096 step:  200 acc_reward:  142.6435933112956 violation_rate:  0.0\n",
      "pole_mass:  0.4487791852069003 pole_length:  0.5805141648644312 step:  200 acc_reward:  130.33284616360768 violation_rate:  0.0\n",
      "pole_mass:  0.6844866754775771 pole_length:  0.5312743456571583 step:  200 acc_reward:  157.04012565606507 violation_rate:  0.0\n",
      "pole_mass:  0.4267697886948963 pole_length:  0.6745802769626132 step:  200 acc_reward:  76.23247226693888 violation_rate:  0.0\n",
      "pole_mass:  0.5759909532116624 pole_length:  0.533011116576154 step:  200 acc_reward:  79.5873506579549 violation_rate:  0.0\n",
      "pole_mass:  0.3791665233937623 pole_length:  0.5001220997769523 step:  200 acc_reward:  176.98076897433472 violation_rate:  0.01\n",
      "pole_mass:  0.4934683205551963 pole_length:  0.6625878825374742 step:  200 acc_reward:  68.45458740101105 violation_rate:  0.0\n",
      "pole_mass:  0.3273325094487252 pole_length:  0.6212255133510689 step:  200 acc_reward:  86.30578323162983 violation_rate:  0.0\n",
      "pole_mass:  0.6370905598120411 pole_length:  0.3115567169908478 step:  200 acc_reward:  62.77007247043473 violation_rate:  0.01\n",
      "pole_mass:  0.5317166111333103 pole_length:  0.6321337492615609 step:  200 acc_reward:  74.56311369948257 violation_rate:  0.01\n",
      "pole_mass:  0.4458926585277589 pole_length:  0.5493217701103452 step:  200 acc_reward:  74.21919291806154 violation_rate:  0.0\n",
      "pole_mass:  0.3758798789088042 pole_length:  0.6541883968019289 step:  200 acc_reward:  60.80546156274194 violation_rate:  0.01\n",
      "pole_mass:  0.3087335993416771 pole_length:  0.5583484357246682 step:  200 acc_reward:  156.49523965767403 violation_rate:  0.0\n",
      "pole_mass:  0.6520166179541611 pole_length:  0.699083861319902 step:  200 acc_reward:  78.81461459195657 violation_rate:  0.0\n",
      "pole_mass:  0.6838200319354835 pole_length:  0.5632268970385135 step:  200 acc_reward:  72.52444552365581 violation_rate:  0.0\n",
      "pole_mass:  0.5669632884332316 pole_length:  0.5147462694978987 step:  200 acc_reward:  165.7791930172412 violation_rate:  0.0\n",
      "pole_mass:  0.37946559639115207 pole_length:  0.362529908277951 step:  200 acc_reward:  83.90417595854159 violation_rate:  0.0\n",
      "pole_mass:  0.6943905346861892 pole_length:  0.4826591976269972 step:  200 acc_reward:  100.6912979037705 violation_rate:  0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pole_mass:  0.6230482371171967 pole_length:  0.4414464027537346 step:  200 acc_reward:  104.41917735339509 violation_rate:  0.0\n",
      "pole_mass:  0.6117215847095708 pole_length:  0.5933934456927377 step:  200 acc_reward:  151.3854980929227 violation_rate:  0.0\n"
     ]
    }
   ],
   "source": [
    "# log_name == None\n",
    "\"\"\"testing the model with MPC while training \"\"\"\n",
    "test_episode = 1\n",
    "test_epoch = 300\n",
    "log = []\n",
    "m_p_list = [0.3, 0.7]\n",
    "l_list = [0.3, 0.7]\n",
    "\n",
    "for ep in range(test_epoch):\n",
    "    for task_idx in range(1):\n",
    "        task_steps = 0\n",
    "        env = task[0]\n",
    "#         m_p = m_p_list[np.random.randint(2)]\n",
    "#         l = l_list[np.random.randint(2)]\n",
    "        m_p = np.random.uniform(0.3, 0.7)\n",
    "        l = np.random.uniform(0.3, 0.7)\n",
    "        env.unwrapped.m_p = m_p\n",
    "        env.unwrapped.l = l\n",
    "        for epi in range(test_episode):\n",
    "            acc_reward = 0\n",
    "            obs = env.reset()\n",
    "            O, A, R, acc_reward, done, V = [], [], [], 0, False, []\n",
    "            mpc_controller.reset()\n",
    "            i = 0\n",
    "            while not done:\n",
    "                i+= 1\n",
    "                env_copy = prepare_dynamics(gym_config)[0]\n",
    "                env_copy.unwrapped.m_p = m_p\n",
    "                env_copy.unwrapped.l = l\n",
    "                env_copy.reset()\n",
    "                if task_steps > 0:\n",
    "                    action = np.array([mpc_controller.act(task=env_copy, model=model, state=obs, ground_truth=True)])\n",
    "                else:\n",
    "                    action = np.array([0.0])\n",
    "                obs_next, reward, done, violation = env.step(action)\n",
    "                task_steps += 1\n",
    "                A.append(action)\n",
    "                O.append(obs_next)\n",
    "                R.append(reward)\n",
    "                V.append(violation)\n",
    "\n",
    "                model.data_process([0, obs, action, obs_next - obs])\n",
    "#                 if task_steps > 2:\n",
    "#                     model.train()\n",
    "                obs = obs_next\n",
    "                acc_reward += reward\n",
    "#             print('task: ', task_idx,'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            print('pole_mass: ', m_p, 'pole_length: ', l, 'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            env.close()\n",
    "\n",
    "            if done:\n",
    "                samples = {\n",
    "                    \"obs\": np.array(O),\n",
    "                    \"actions\": np.array(A),\n",
    "                    \"rewards\": np.array(R),\n",
    "                    \"reward_sum\": acc_reward,\n",
    "                    \"violation_rate\": sum(V)/len(V)\n",
    "                }\n",
    "                log.append(samples)\n",
    "                if log_name is None:\n",
    "                    log_name = datetime.datetime.now()\n",
    "                path = './misc/log/np_adaptation' + log_name.strftime(\"%d-%H-%M\") + '.npy'\n",
    "#                 print(path)\n",
    "                np.save(path, log, allow_pickle=True)\n",
    "                dumb_reward_plot(path)\n",
    "        model.reset()\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4312a0e4bc4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./misc/log/robust_model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d-%H-%M\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.model.state_dict(), './misc/log/robust_model_' + log_name.strftime(\"%d-%H-%M\") + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./misc/log/np_adaptation06-16-13.npy\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  155.37174407349997 violation_rate:  0.035\n",
      "pole_mass:  0.3 pole_length:  0.3 step:  200 acc_reward:  152.98799105235113 violation_rate:  0.09\n",
      "pole_mass:  0.7 pole_length:  0.3 step:  200 acc_reward:  111.36387528293744 violation_rate:  0.0\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  96.15357172916583 violation_rate:  0.285\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  155.69317472118897 violation_rate:  0.095\n",
      "pole_mass:  0.7 pole_length:  0.7 step:  200 acc_reward:  158.11743615910416 violation_rate:  0.035\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  137.99629515002746 violation_rate:  0.235\n",
      "pole_mass:  0.7 pole_length:  0.3 step:  200 acc_reward:  97.34392621617955 violation_rate:  0.0\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  123.25752336064429 violation_rate:  0.22\n",
      "pole_mass:  0.7 pole_length:  0.7 step:  200 acc_reward:  173.97169360055324 violation_rate:  0.0\n"
     ]
    }
   ],
   "source": [
    "config = load_config('config/config_swingup_robust.yml')\n",
    "mpc_config = config['mpc_config']\n",
    "mpc_controller = MPC(mpc_config=mpc_config)\n",
    "\"\"\"testing the model with MPC while training \"\"\"\n",
    "test_episode = 1\n",
    "test_epoch = 10\n",
    "log = []\n",
    "m_p_list = [0.3, 0.7]\n",
    "l_list = [0.3, 0.7]\n",
    "\n",
    "for ep in range(test_epoch):\n",
    "    for task_idx in range(1):\n",
    "        task_steps = 0\n",
    "        env = task[0]\n",
    "        m_p = m_p_list[np.random.randint(2)]\n",
    "        l = l_list[np.random.randint(2)]\n",
    "        env.unwrapped.m_p = m_p\n",
    "        env.unwrapped.l = l\n",
    "        for epi in range(test_episode):\n",
    "            acc_reward = 0\n",
    "            obs = env.reset()\n",
    "            O, A, R, acc_reward, done, V = [], [], [], 0, False, []\n",
    "            mpc_controller.reset()\n",
    "            i = 0\n",
    "            while not done:\n",
    "                i+= 1\n",
    "                env_copy = prepare_dynamics(gym_config)[0]\n",
    "                env_copy.unwrapped.m_p = m_p\n",
    "                env_copy.unwrapped.l = l\n",
    "                env_copy.reset()\n",
    "                if task_steps > 0:\n",
    "                    action = np.array([mpc_controller.act(task=env_copy, model=model, state=obs, ground_truth=True)])\n",
    "                else:\n",
    "                    action = np.array([0.0])\n",
    "                obs_next, reward, done, violation = env.step(action)\n",
    "                task_steps += 1\n",
    "                A.append(action)\n",
    "                O.append(obs_next)\n",
    "                R.append(reward)\n",
    "                V.append(violation)\n",
    "\n",
    "                model.data_process([0, obs, action, obs_next - obs])\n",
    "                obs = obs_next\n",
    "                acc_reward += reward\n",
    "#             print('task: ', task_idx,'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            print('pole_mass: ', m_p, 'pole_length: ', l, 'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            env.close()\n",
    "\n",
    "            if done:\n",
    "                samples = {\n",
    "                    \"obs\": np.array(O),\n",
    "                    \"actions\": np.array(A),\n",
    "                    \"rewards\": np.array(R),\n",
    "                    \"reward_sum\": acc_reward,\n",
    "                    \"violation_rate\": sum(V)/len(V)\n",
    "                }\n",
    "                log.append(samples)\n",
    "#                 if log_name is None:\n",
    "#                     log_name = datetime.datetime.now()\n",
    "#                 path = './misc/log/np_adaptation' + log_name.strftime(\"%d-%H-%M\") + '.npy'\n",
    "#                 np.save(path, log, allow_pickle=True)\n",
    "#                 dumb_reward_plot(path)\n",
    "            \n",
    "        model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pole_mass:  0.3 pole_length:  0.3 step:  200 acc_reward:  63.502598919684274 violation_rate:  0.085\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  91.83178738314267 violation_rate:  0.075\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  76.59254432443666 violation_rate:  0.045\n",
      "pole_mass:  0.7 pole_length:  0.3 step:  200 acc_reward:  51.18483559010456 violation_rate:  0.095\n",
      "pole_mass:  0.7 pole_length:  0.3 step:  200 acc_reward:  41.23595679727575 violation_rate:  0.0\n",
      "pole_mass:  0.3 pole_length:  0.3 step:  200 acc_reward:  62.29221715500605 violation_rate:  0.06\n",
      "pole_mass:  0.3 pole_length:  0.3 step:  200 acc_reward:  68.62416004089802 violation_rate:  0.07\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  112.86228850228765 violation_rate:  0.005\n",
      "pole_mass:  0.3 pole_length:  0.7 step:  200 acc_reward:  59.913114562337505 violation_rate:  0.05\n",
      "pole_mass:  0.7 pole_length:  0.7 step:  200 acc_reward:  35.878112404024925 violation_rate:  0.04\n"
     ]
    }
   ],
   "source": [
    "# only_prior_model\n",
    "test_episode = 1\n",
    "test_epoch = 10\n",
    "log = []\n",
    "m_p_list = [0.3, 0.7]\n",
    "l_list = [0.3, 0.7]\n",
    "\n",
    "for ep in range(test_epoch):\n",
    "    for task_idx in range(1):\n",
    "        task_steps = 0\n",
    "        env = task[0]\n",
    "        m_p = m_p_list[np.random.randint(2)]\n",
    "        l = l_list[np.random.randint(2)]\n",
    "        env.unwrapped.m_p = m_p\n",
    "        env.unwrapped.l = l\n",
    "        for epi in range(test_episode):\n",
    "            acc_reward = 0\n",
    "            obs = env.reset()\n",
    "            O, A, R, acc_reward, done, V = [], [], [], 0, False, []\n",
    "            mpc_controller.reset()\n",
    "            i = 0\n",
    "            while not done:\n",
    "                i+= 1\n",
    "                env_copy = prepare_dynamics(gym_config)[0]\n",
    "                env_copy.unwrapped.m_p = m_p\n",
    "                env_copy.unwrapped.l = l\n",
    "                env_copy.reset()\n",
    "                if task_steps > 0:\n",
    "                    action = np.array([mpc_controller.act(task=env_copy, model=model, state=obs, ground_truth=True)])\n",
    "                else:\n",
    "                    action = np.array([0.0])\n",
    "                obs_next, reward, done, violation = env.step(action)\n",
    "                task_steps += 1\n",
    "                A.append(action)\n",
    "                O.append(obs_next)\n",
    "                R.append(reward)\n",
    "                V.append(violation)\n",
    "\n",
    "                model.data_process([0, obs, action, obs_next - obs])\n",
    "                obs = obs_next\n",
    "                acc_reward += reward\n",
    "#             print('task: ', task_idx,'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            print('pole_mass: ', m_p, 'pole_length: ', l, 'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "            env.close()\n",
    "\n",
    "            if done:\n",
    "                samples = {\n",
    "                    \"obs\": np.array(O),\n",
    "                    \"actions\": np.array(A),\n",
    "                    \"rewards\": np.array(R),\n",
    "                    \"reward_sum\": acc_reward,\n",
    "                    \"violation_rate\": sum(V)/len(V)\n",
    "                }\n",
    "                log.append(samples)\n",
    "#                 if log_name is None:\n",
    "#                     log_name = datetime.datetime.now()\n",
    "#                 path = './misc/log/np_adaptation' + log_name.strftime(\"%d-%H-%M\") + '.npy'\n",
    "#                 np.save(path, log, allow_pickle=True)\n",
    "#                 dumb_reward_plot(path)\n",
    "            \n",
    "        model.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "a= [0, 1, 2]\n",
    "s = random.sample(a, 2)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([4, 5, 6], [1, 2, 3])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "a = deque(maxlen=100)\n",
    "xx = [1,2,3]\n",
    "yy = [4,5,6]\n",
    "a.append((xx,yy))\n",
    "a.append((yy,xx))\n",
    "s = random.sample(a, k=1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y) = s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
