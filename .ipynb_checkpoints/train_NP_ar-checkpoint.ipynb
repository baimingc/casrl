{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "import yaml\n",
    "from utils import dumb_reward_plot\n",
    "import gym\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import assistive_gym\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "from utils import plot_reward, plot_index\n",
    "from mpc.mpc_ar import MPC\n",
    "from baselines.NP_epi import NP\n",
    "\n",
    "def load_config(config_path=\"config.yml\"):\n",
    "    if os.path.isfile(config_path):\n",
    "        f = open(config_path)\n",
    "        return yaml.load(f, Loader=yaml.FullLoader)\n",
    "    else:\n",
    "        raise Exception(\"Configuration file is not found in the path: \"+config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-07 13:51:02.753 | INFO     | __main__:<module>:9 - Using model: NP\n",
      "Using TensorFlow backend.\n",
      "/home/baiming/anaconda3/envs/robustnp/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/baiming/anaconda3/envs/robustnp/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/baiming/anaconda3/envs/robustnp/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/baiming/anaconda3/envs/robustnp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# config = load_config('config/config_cpstable_np.yml')\n",
    "config = load_config('config/config_ar_np.yml')\n",
    "mpc_config = config['mpc_config']\n",
    "gym_config = config['gym_config']\n",
    "render = gym_config['render']\n",
    "np_config = config['NP_config']\n",
    "\n",
    "model = NP(NP_config=np_config)\n",
    "logger.info('Using model: {}', model.name)\n",
    "\n",
    "mpc_controller = MPC(mpc_config=mpc_config)\n",
    "\n",
    "env = gym.make(\"FeedingJacoHuman-v0\")\n",
    "log_name = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NP pretrain\"\"\"\n",
    "\n",
    "pretrain_episodes = 1\n",
    "for task_idx in range(1):\n",
    "    a1 = np.random.uniform(-0.2, 0.2)\n",
    "    a2 = np.random.uniform(-0.2, 0.2)\n",
    "    for epi in range(pretrain_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        mpc_controller.reset()\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            action[-4] = a1\n",
    "            action[-1] = a2\n",
    "            obs_next, reward, done, _ = env.step(action)\n",
    "            model.data_process([0, obs, action[:7], obs_next - obs])\n",
    "            obs = obs_next\n",
    "    model.reset()\n",
    "    model.train()\n",
    "# torch.save(model.model.state_dict(), './misc/log/model_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 acc_reward:  -160.36873226984065 success_food 0 spilled_food 8 distance 0.5719907827905848\n",
      "1 acc_reward:  -150.19017477551782 success_food 0 spilled_food 8 distance 0.41858066268226823\n"
     ]
    }
   ],
   "source": [
    "# log_name == None\n",
    "\n",
    "\n",
    "\"\"\"testing the model with MPC while training \"\"\"\n",
    "test_episode = 1\n",
    "test_epoch = 300\n",
    "save_every = 5\n",
    "log = []\n",
    "model.reset()\n",
    "for ep in range(test_epoch):\n",
    "    task_steps = 0\n",
    "    a1 = np.random.uniform(-0.2, 0.2)\n",
    "    a2 = np.random.uniform(-0.2, 0.2)\n",
    "    human_action = np.array([a1, 0, 0, a2])\n",
    "    for epi in range(test_episode):\n",
    "        acc_reward = 0\n",
    "        obs = env.reset()\n",
    "        O, A, R, acc_reward, done, V = [], [], [], 0, False, []\n",
    "        mpc_controller.reset()\n",
    "        while not done:\n",
    "            if task_steps > 0:\n",
    "                robot_action = mpc_controller.act(model=model, state=obs)\n",
    "            else:\n",
    "                robot_action = np.zeros(7)\n",
    "            action = np.concatenate((robot_action, human_action))\n",
    "            obs_next, reward, done, info = env.step(action)\n",
    "            task_steps += 1\n",
    "            A.append(action)\n",
    "            O.append(obs_next)\n",
    "            R.append(reward)\n",
    "            model.data_process([0, obs, action[:7], obs_next - obs])\n",
    "            obs = obs_next\n",
    "            acc_reward += reward\n",
    "    #             print('task: ', task_idx,'step: ', i, 'acc_reward: ', acc_reward, 'violation_rate: ', sum(V)/len(V))\n",
    "        print(ep, 'acc_reward: ', acc_reward, 'success_food',info['task_success'], 'spilled_food',info['spilled_food'], 'distance', np.linalg.norm(obs[6:9]))\n",
    "        if ep % save_every == save_every -1:\n",
    "            torch.save(model.model.state_dict(), './log/ar_np_ju_{}.pth'.format(ep))\n",
    "            if log_name is None:\n",
    "                log_name = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "            torch.save(model.model.state_dict(), './log/ar_np_ju_{}.pth'.format(ep))\n",
    "            print('model saved at'+log_name)\n",
    "#         if log_name is None:\n",
    "#             log_name = datetime.datetime.now()\n",
    "#             path = './misc/log/np_adaptation' + log_name.strftime(\"%d-%H-%M\") + '.npy'\n",
    "#     #                 print(path)\n",
    "#             np.save(path, log, allow_pickle=True)\n",
    "#             dumb_reward_plot(path)\n",
    "    model.reset()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.model.state_dict(), './misc/log/robust_model_latent-' + log_name.strftime(\"%d-%H-%M\") + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
