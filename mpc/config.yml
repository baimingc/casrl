model_config:
    load_model: False # If set true, you must specify the model path, otherwise train a new model
    model_path: "storage/example.ckpt" # the path to load the model
    n_states: 2 # environment states
    n_actions: 1 # how many controls we need
    n_hidden: 1 # hidden layer number
    size_hidden: 128 # hidden layer size
    use_cuda: True

training_config:
    n_epochs: 500 # how many epoches to train the dynamic model
    learning_rate: 0.008
    batch_size: 512
    save_model_flag: True
    save_model_path: "storage/exp_1.ckpt" # the path to save the model
    save_loss_fig: True
    save_loss_fig_frequency: 10 # how many every epochs to save the loss figure 
    exp_number: 1  # experiment number

dataset_config:
    load_flag: False 
    load_path: "storage/data_exp_1.pkl"     
    n_max_steps: 1000 # maximum steps per episode
    n_random_episodes: 800 # how many random episodes' data to fit the initial dynamic model
    testset_split: 0.2 # testset's portion in the random dataset, the rest portion is the training set
    n_mpc_episodes: 4 # how many episodes data sampled with the MPC controller
    mpc_dataset_split: 0.5 # mpc dataset's portion in the training set
    min_train_samples: 6000
    n_mpc_itrs: 100 # the number to perform reinforce iteration
    save_flag: True # set True if you want to save all the dataset
    save_path: "storage/data_exp_1.pkl"

# MPC controller configuration
mpc_config:
    optimizer: "Random"  #Random or CEM
    Random:
        horizon: 8 # how long of the horizon to predict
        popsize: 100000 # how many random samples for mpc
        gamma: 0.99 # reward discount coefficient
        action_low: -2 # lower bound of the solution space
        action_high: 2 # upper bound of the solution space
        max_iters: 20
        num_elites: 50
        epsilon: 0.2
        alpha: 0.01
        init_mean: 0
        init_var: 10
    CEM:
        horizon: 8 # how long of the horizon to predict
        popsize: 500 # how many random samples for mpc
        gamma: 0.99 # reward discount coefficient
        action_low: -2 # lower bound of the solution space
        action_high: 2 # upper bound of the solution space
        max_iters: 25
        num_elites: 5
        epsilon: 0.02
        alpha: 0.10
        init_mean: 0
        init_var: 10
